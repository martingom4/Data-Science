{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"../Apoyo-Desafio/Telco-Customer-Churn.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir las columnas categ√≥ricas\n",
    "categorical_columns = ['customerID', 'gender', 'Partner', 'Dependents', 'PhoneService',\n",
    "\t\t\t\t\t   'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup',\n",
    "\t\t\t\t\t   'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies',\n",
    "\t\t\t\t\t   'Contract', 'PaperlessBilling', 'PaymentMethod', 'Churn']\n",
    "\n",
    "# Definir las columnas num√©ricas\n",
    "numerical_columns = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "\n",
    "# Generar estad√≠sticas descriptivas para variables num√©ricas\n",
    "numerical_stats = df.describe()\n",
    "print(\"Estad√≠sticas descriptivas para variables num√©ricas:\")\n",
    "print(numerical_stats)\n",
    "\n",
    "# Generar estad√≠sticas descriptivas para variables categ√≥ricas\n",
    "categorical_stats = df[categorical_columns].describe()\n",
    "print(\"\\nEstad√≠sticas descriptivas para variables categ√≥ricas:\")\n",
    "print(categorical_stats)\n",
    "\n",
    "# Visualizar la distribuci√≥n de las variables clave mediante histogramas\n",
    "df.hist(bins=30, figsize=(20, 15))\n",
    "plt.suptitle(\"Histogramas de variables num√©ricas\")\n",
    "plt.show()\n",
    "\n",
    "# Visualizar la distribuci√≥n de las variables clave mediante diagramas de caja\n",
    "df.plot(kind='box', subplots=True, layout=(6, 4), figsize=(20, 20), sharex=False, sharey=False)\n",
    "plt.suptitle(\"Diagramas de caja de variables num√©ricas\")\n",
    "plt.show()\n",
    "\n",
    "# An√°lisis de correlaciones entre variables\n",
    "correlation_matrix = df.corr()\n",
    "print(\"\\nMatriz de correlaci√≥n:\")\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Visualizar la matriz de correlaci√≥n mediante un mapa de calor\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title(\"Mapa de calor de la matriz de correlaci√≥n\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Resumen de hallazgos clave\n",
    "print(\"\\nConclusiones:\")\n",
    "print(\"1. Estad√≠sticas descriptivas muestran la media, mediana, moda, desviaci√≥n est√°ndar y percentiles de las variables num√©ricas.\")\n",
    "print(\"2. Histogramas y diagramas de caja revelan la distribuci√≥n de las variables y posibles valores at√≠picos.\")\n",
    "print(\"3. Gr√°ficos de dispersi√≥n ayudan a identificar relaciones entre pares de variables.\")\n",
    "print(\"4. La matriz de correlaci√≥n y el mapa de calor muestran las correlaciones entre variables.\")\n",
    "print(\"5. Valores at√≠picos identificados deben ser tratados seg√∫n el contexto del an√°lisis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  An√°lisis Exploratorio de Datos (EDA)\n",
    "\n",
    "El objetivo de esta secci√≥n es explorar las caracter√≠sticas del dataset, identificar patrones y detectar posibles problemas en los datos antes de entrenar los modelos de machine learning. \n",
    "\n",
    "## **1Ô∏è Definici√≥n de Variables**\n",
    "\n",
    "Para facilitar el an√°lisis, clasificamos las variables en dos grupos:\n",
    "\n",
    "- **Variables Categ√≥ricas:** Incluyen informaci√≥n cualitativa sobre los clientes, como g√©nero, tipo de contrato y servicios contratados.\n",
    "- **Variables Num√©ricas:** Contienen informaci√≥n cuantitativa, como duraci√≥n de la suscripci√≥n, cargos mensuales y cargos totales.\n",
    "\n",
    "---\n",
    "\n",
    "## **2Ô∏è An√°lisis Estad√≠stico de las Variables**\n",
    "Antes de aplicar cualquier modelo, es fundamental entender la distribuci√≥n de los datos mediante estad√≠sticas descriptivas.\n",
    "\n",
    "- **Variables num√©ricas:** Se analiza la media, mediana, desviaci√≥n est√°ndar y percentiles.\n",
    "- **Variables categ√≥ricas:** Se revisa la cantidad de observaciones por cada categor√≠a.\n",
    "\n",
    "---\n",
    "\n",
    "## **3Ô∏è Visualizaci√≥n de la Distribuci√≥n de los Datos**\n",
    "Para comprender la dispersi√≥n y la estructura de los datos, generamos diferentes visualizaciones:\n",
    "\n",
    "###  **Histogramas**\n",
    "Los histogramas permiten analizar la distribuci√≥n de cada variable num√©rica y detectar sesgos en los datos.\n",
    "\n",
    "###  **Diagramas de Caja (Boxplots)**\n",
    "Los boxplots ayudan a identificar valores at√≠picos y visualizar la dispersi√≥n de los datos.\n",
    "\n",
    "---\n",
    "\n",
    "## **4Ô∏è An√°lisis de Correlaci√≥n entre Variables**\n",
    "Para evaluar relaciones entre variables y posibles colinealidades, generamos:\n",
    "\n",
    "- **Matriz de correlaci√≥n:** Muestra el coeficiente de correlaci√≥n entre las variables num√©ricas.\n",
    "- **Mapa de calor:** Representaci√≥n visual de la matriz de correlaci√≥n con `seaborn.heatmap`.\n",
    "\n",
    "---\n",
    "\n",
    "## **5Ô∏è Conclusiones del An√°lisis Exploratorio**\n",
    "A partir de los gr√°ficos y estad√≠sticas obtenidos, podemos destacar los siguientes puntos clave:\n",
    "\n",
    "1. **Las estad√≠sticas descriptivas** nos permiten ver la media, mediana, moda y dispersi√≥n de los datos.\n",
    "2. **Histogramas y diagramas de caja** ayudan a identificar valores extremos y patrones de distribuci√≥n.\n",
    "3. **Los gr√°ficos de dispersi√≥n** pueden mostrar relaciones entre pares de variables relevantes.\n",
    "4. **La matriz de correlaci√≥n** indica posibles relaciones entre caracter√≠sticas.\n",
    "5. **Valores at√≠picos** deben ser analizados y tratados adecuadamente antes del modelado.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar valores nulos, duplicados y tipos de datos\n",
    "print(\"Valores nulos por columna:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\nValores duplicados:\")\n",
    "print(df.duplicated().sum())\n",
    "print(\"\\nTipos de datos:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Limpieza de datos\n",
    "# Eliminar valores duplicados\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Imputar valores nulos\n",
    "df['TotalCharges'] = df['TotalCharges'].replace(\" \", np.nan).astype(float)\n",
    "df['TotalCharges'] = df['TotalCharges'].fillna(df['TotalCharges'].mean())\n",
    "\n",
    "# Transformaci√≥n de datos\n",
    "# Convertir variables categ√≥ricas en variables num√©ricas\n",
    "label_encoders = {}\n",
    "for column in categorical_columns:\n",
    "    if df[column].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        df[column] = le.fit_transform(df[column])\n",
    "        label_encoders[column] = le\n",
    "\n",
    "# Normalizaci√≥n y estandarizaci√≥n\n",
    "scaler = StandardScaler()\n",
    "df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
    "\n",
    "# Informe de los pasos realizados\n",
    "print(\"\\nInforme de limpieza y transformaci√≥n:\")\n",
    "print(\"1. Valores duplicados eliminados.\")\n",
    "print(\"2. Valores nulos imputados en la columna 'TotalCharges'.\")\n",
    "print(\"3. Variables categ√≥ricas convertidas en variables num√©ricas.\")\n",
    "print(\"4. Variables num√©ricas normalizadas usando Z-score Normalization.\")\n",
    "\n",
    "# Visualizaci√≥n de datos antes y despu√©s del preprocesamiento\n",
    "print(\"\\nDatos antes del preprocesamiento:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nDatos despu√©s del preprocesamiento:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Preprocesamiento de Datos\n",
    "\n",
    "En esta secci√≥n, me encargo de **limpiar y transformar** los datos para asegurar que est√©n en el formato adecuado antes de entrenar cualquier modelo de machine learning.\n",
    "\n",
    "---\n",
    "\n",
    "## **1Ô∏è Identificaci√≥n de Problemas en los Datos**\n",
    "Antes de aplicar transformaciones, lo primero que hago es revisar la calidad de los datos. Para ello, verifico:\n",
    "\n",
    "- **Valores nulos**: Identifico si hay datos faltantes en alguna columna.\n",
    "- **Valores duplicados**: Detecto si hay filas repetidas que podr√≠an afectar el an√°lisis.\n",
    "- **Tipos de datos**: Me aseguro de que cada variable tenga el formato adecuado (n√∫meros, categor√≠as, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "## **2Ô∏è Limpieza de Datos**\n",
    "###  Eliminaci√≥n de Duplicados  \n",
    "Si encuentro filas duplicadas en el dataset, las elimino para evitar que influyan negativamente en el modelo.\n",
    "\n",
    "###  Imputaci√≥n de Valores Nulos  \n",
    "En la columna `TotalCharges`, detect√© algunos valores vac√≠os representados como `\" \"`. Lo que hago es:\n",
    "\n",
    "1. Convertir esos valores en `NaN` para tratarlos correctamente.\n",
    "2. Rellenar los valores `NaN` con la **media** de la columna, asegurando que no haya datos faltantes.\n",
    "\n",
    "---\n",
    "\n",
    "## **3Ô∏è Transformaci√≥n de Datos**\n",
    "###  Conversi√≥n de Variables Categ√≥ricas  \n",
    "Dado que los modelos de machine learning no pueden manejar directamente variables categ√≥ricas, convierto estas variables en valores num√©ricos utilizando `LabelEncoder`.  \n",
    "\n",
    "Cada categor√≠a recibe un n√∫mero √∫nico, lo que permite que los modelos interpreten la informaci√≥n correctamente.\n",
    "\n",
    "---\n",
    "\n",
    "## **4Ô∏è Normalizaci√≥n y Estandarizaci√≥n**\n",
    "Para que los modelos trabajen mejor con las variables num√©ricas (`tenure`, `MonthlyCharges`, `TotalCharges`), aplico `StandardScaler`.  \n",
    "Esto **normaliza** los valores usando la t√©cnica **Z-score Normalization**, que centra los datos en media 0 y varianza 1.  \n",
    "\n",
    " **Ventaja:** Evita que una variable con valores muy grandes (ejemplo: `TotalCharges`) domine sobre otras con valores m√°s peque√±os.\n",
    "\n",
    "---\n",
    "\n",
    "## **5Ô∏è Resumen del Preprocesamiento**\n",
    "Despu√©s de aplicar los pasos anteriores, genero un informe de los cambios realizados:\n",
    "\n",
    "1. **Valores duplicados eliminados.**\n",
    "2. **Valores nulos imputados en la columna `TotalCharges`.**\n",
    "3. **Variables categ√≥ricas convertidas en valores num√©ricos.**\n",
    "4. **Variables num√©ricas normalizadas usando `Z-score Normalization`.**\n",
    "\n",
    "---\n",
    "\n",
    "## **6Ô∏è Visualizaci√≥n de los Datos Antes y Despu√©s**\n",
    "Finalmente, muestro un resumen del dataset antes y despu√©s del preprocesamiento para asegurarme de que las transformaciones se aplicaron correctamente.\n",
    "\n",
    "Este proceso es clave para garantizar que los datos sean adecuados para el entrenamiento de modelos de machine learning y evitar sesgos que puedan afectar el rendimiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Definir las caracter√≠sticas (X) y la variable objetivo (y)\n",
    "X = df.drop(columns=['Churn', 'customerID'])\n",
    "y = df['Churn']\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f'Tama√±o del conjunto de entrenamiento: {X_train.shape[0]}')\n",
    "print(f'Tama√±o del conjunto de prueba: {X_test.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Definir los modelos y sus hiperpar√°metros\n",
    "models = {\n",
    "    'RandomForest': {\n",
    "        'model': RandomForestClassifier(random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [None, 10, 20, 30],\n",
    "            'min_samples_split': [2, 5, 10]\n",
    "        }\n",
    "    },\n",
    "    'GradientBoosting': {\n",
    "        'model': GradientBoostingClassifier(random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'max_depth': [3, 5, 7]\n",
    "        }\n",
    "    },\n",
    "    'SVC': {\n",
    "        'model': SVC(random_state=42),\n",
    "        'params': {\n",
    "            'C': [0.1, 1, 10],\n",
    "            'kernel': ['linear', 'rbf', 'poly'],\n",
    "            'gamma': ['scale', 'auto']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Entrenar y optimizar los modelos\n",
    "best_models = {}\n",
    "for name, model_info in models.items():\n",
    "    grid_search = GridSearchCV(model_info['model'], model_info['params'], cv=5, n_jobs=-1, scoring='accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_models[name] = grid_search.best_estimator_\n",
    "    print(f\"Best parameters for {name}: {grid_search.best_params_}\")\n",
    "    print(f\"Best score for {name}: {grid_search.best_score_}\")\n",
    "\n",
    "# Evaluar los mejores modelos en el conjunto de prueba\n",
    "for name, model in best_models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"Classification report for {name}:\\n{classification_report(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Entrenamiento y Optimizaci√≥n de Modelos de Machine Learning\n",
    "\n",
    "En esta secci√≥n, me encargo de **definir, entrenar y optimizar** varios modelos de clasificaci√≥n para predecir la fuga de clientes (*churn*). Utilizo **b√∫squeda de hiperpar√°metros** para obtener el mejor rendimiento posible.\n",
    "\n",
    "---\n",
    "\n",
    "## **1Ô∏è Selecci√≥n de Modelos**\n",
    "Para abordar este problema, seleccion√© tres algoritmos de clasificaci√≥n populares:\n",
    "\n",
    "- **Random Forest:** Modelo basado en m√∫ltiples √°rboles de decisi√≥n, ideal para manejar datos con relaciones no lineales.\n",
    "- **Gradient Boosting:** Algoritmo de boosting que ajusta errores progresivamente para mejorar la precisi√≥n.\n",
    "- **SVM (Support Vector Machine):** Modelo basado en la maximizaci√≥n de m√°rgenes entre clases.\n",
    "\n",
    "Cada uno de estos modelos tiene ventajas espec√≠ficas:\n",
    "‚úÖ **Random Forest** es robusto y maneja bien datos con muchas caracter√≠sticas.  \n",
    "‚úÖ **Gradient Boosting** suele lograr alta precisi√≥n optimizando el error en cada iteraci√≥n.  \n",
    "‚úÖ **SVM** es √∫til cuando los datos no son linealmente separables y requiere pocos par√°metros de ajuste.  \n",
    "\n",
    "---\n",
    "\n",
    "## **2Ô∏è Definici√≥n de Hiperpar√°metros**\n",
    "Para cada modelo, defino una serie de hiperpar√°metros que quiero optimizar:\n",
    "\n",
    "- **Random Forest:**\n",
    "  - `n_estimators`: N√∫mero de √°rboles en el bosque.\n",
    "  - `max_depth`: Profundidad m√°xima de cada √°rbol.\n",
    "  - `min_samples_split`: M√≠nimo de muestras necesarias para dividir un nodo.\n",
    "\n",
    "- **Gradient Boosting:**\n",
    "  - `n_estimators`: N√∫mero de √°rboles a entrenar.\n",
    "  - `learning_rate`: Controla cu√°nto cambia el modelo con cada iteraci√≥n.\n",
    "  - `max_depth`: Profundidad de los √°rboles en la fase de boosting.\n",
    "\n",
    "- **SVC (Support Vector Machine):**\n",
    "  - `C`: Par√°metro de regularizaci√≥n (m√°s alto, menos penalizaci√≥n).\n",
    "  - `kernel`: Define la funci√≥n del hiperplano (lineal, polinomial o radial).\n",
    "  - `gamma`: Ajusta la influencia de los puntos de entrenamiento.\n",
    "\n",
    "Estos hiperpar√°metros son clave porque afectan directamente el rendimiento del modelo.\n",
    "\n",
    "---\n",
    "\n",
    "## **3Ô∏è Entrenamiento y Optimizaci√≥n con GridSearchCV**\n",
    "Para encontrar los mejores hiperpar√°metros, utilizo **GridSearchCV**, que prueba m√∫ltiples combinaciones y selecciona la mejor configuraci√≥n.  \n",
    "\n",
    " **Ventaja de GridSearchCV:** Eval√∫a autom√°ticamente cada combinaci√≥n con validaci√≥n cruzada (`cv=5`), lo que asegura que el modelo no est√© sobreajustado.\n",
    "\n",
    "El proceso consiste en:\n",
    "1. Definir los modelos y sus hiperpar√°metros.\n",
    "2. Aplicar `GridSearchCV` para buscar la mejor combinaci√≥n.\n",
    "3. Guardar el mejor modelo para cada algoritmo.\n",
    "\n",
    "Despu√©s de entrenar cada modelo, imprimo los **mejores hiperpar√°metros** y la **mejor puntuaci√≥n** obtenida durante la validaci√≥n cruzada.\n",
    "\n",
    "---\n",
    "\n",
    "## **4Ô∏è Evaluaci√≥n de los Modelos**\n",
    "Una vez que tengo los modelos optimizados, los pruebo en el **conjunto de prueba (`X_test`)** para evaluar su rendimiento real.\n",
    "\n",
    "Para esto, uso `classification_report`, que me proporciona m√©tricas clave como:\n",
    "- **Precisi√≥n (`precision`)**: Cu√°ntos de los clientes que predije como \"fugados\" realmente lo son.\n",
    "- **Sensibilidad (`recall`)**: Cu√°ntos de los clientes fugados reales fueron detectados correctamente.\n",
    "- **F1-score**: Un balance entre precisi√≥n y sensibilidad.\n",
    "- **Exactitud (`accuracy`)**: Porcentaje total de predicciones correctas.\n",
    "\n",
    "Cada modelo genera un informe con estas m√©tricas, lo que me permite comparar su desempe√±o y elegir el m√°s adecuado.\n",
    "\n",
    "---\n",
    "\n",
    "## **5Ô∏è Conclusi√≥n**\n",
    "Despu√©s de evaluar los modelos, puedo determinar cu√°l es el mejor predictor para detectar clientes con riesgo de fuga.\n",
    "\n",
    " **Posibles mejoras futuras:**\n",
    "- Evaluar otros modelos como **XGBoost** o **Redes Neuronales**.\n",
    "- Usar t√©cnicas de balanceo de clases si los datos est√°n desbalanceados (`SMOTE`).\n",
    "- Aplicar `RandomizedSearchCV` para acelerar la b√∫squeda de hiperpar√°metros.\n",
    "\n",
    "Este proceso garantiza que obtenga el modelo m√°s eficiente para ayudar a la empresa a reducir la fuga de clientes y mejorar la retenci√≥n. üöÄüî•\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Evaluar los mejores modelos en el conjunto de prueba\n",
    "for name, model in best_models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Metrics for {name}:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
