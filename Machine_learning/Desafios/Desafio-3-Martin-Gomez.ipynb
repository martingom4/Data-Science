{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"../Apoyo-Desafio/Telco-Customer-Churn.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir las columnas categóricas\n",
    "categorical_columns = ['customerID', 'gender', 'Partner', 'Dependents', 'PhoneService',\n",
    "\t\t\t\t\t   'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup',\n",
    "\t\t\t\t\t   'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies',\n",
    "\t\t\t\t\t   'Contract', 'PaperlessBilling', 'PaymentMethod', 'Churn']\n",
    "\n",
    "# Definir las columnas numéricas\n",
    "numerical_columns = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "\n",
    "# Generar estadísticas descriptivas para variables numéricas\n",
    "numerical_stats = df.describe()\n",
    "print(\"Estadísticas descriptivas para variables numéricas:\")\n",
    "print(numerical_stats)\n",
    "\n",
    "# Generar estadísticas descriptivas para variables categóricas\n",
    "categorical_stats = df[categorical_columns].describe()\n",
    "print(\"\\nEstadísticas descriptivas para variables categóricas:\")\n",
    "print(categorical_stats)\n",
    "\n",
    "# Visualizar la distribución de las variables clave mediante histogramas\n",
    "df.hist(bins=30, figsize=(20, 15))\n",
    "plt.suptitle(\"Histogramas de variables numéricas\")\n",
    "plt.show()\n",
    "\n",
    "# Visualizar la distribución de las variables clave mediante diagramas de caja\n",
    "df.plot(kind='box', subplots=True, layout=(6, 4), figsize=(20, 20), sharex=False, sharey=False)\n",
    "plt.suptitle(\"Diagramas de caja de variables numéricas\")\n",
    "plt.show()\n",
    "\n",
    "# Análisis de correlaciones entre variables\n",
    "correlation_matrix = df.corr()\n",
    "print(\"\\nMatriz de correlación:\")\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Visualizar la matriz de correlación mediante un mapa de calor\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title(\"Mapa de calor de la matriz de correlación\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Resumen de hallazgos clave\n",
    "print(\"\\nConclusiones:\")\n",
    "print(\"1. Estadísticas descriptivas muestran la media, mediana, moda, desviación estándar y percentiles de las variables numéricas.\")\n",
    "print(\"2. Histogramas y diagramas de caja revelan la distribución de las variables y posibles valores atípicos.\")\n",
    "print(\"3. Gráficos de dispersión ayudan a identificar relaciones entre pares de variables.\")\n",
    "print(\"4. La matriz de correlación y el mapa de calor muestran las correlaciones entre variables.\")\n",
    "print(\"5. Valores atípicos identificados deben ser tratados según el contexto del análisis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Análisis Exploratorio de Datos (EDA)\n",
    "\n",
    "El objetivo de esta sección es explorar las características del dataset, identificar patrones y detectar posibles problemas en los datos antes de entrenar los modelos de machine learning. \n",
    "\n",
    "## **1️ Definición de Variables**\n",
    "\n",
    "Para facilitar el análisis, clasificamos las variables en dos grupos:\n",
    "\n",
    "- **Variables Categóricas:** Incluyen información cualitativa sobre los clientes, como género, tipo de contrato y servicios contratados.\n",
    "- **Variables Numéricas:** Contienen información cuantitativa, como duración de la suscripción, cargos mensuales y cargos totales.\n",
    "\n",
    "---\n",
    "\n",
    "## **2️ Análisis Estadístico de las Variables**\n",
    "Antes de aplicar cualquier modelo, es fundamental entender la distribución de los datos mediante estadísticas descriptivas.\n",
    "\n",
    "- **Variables numéricas:** Se analiza la media, mediana, desviación estándar y percentiles.\n",
    "- **Variables categóricas:** Se revisa la cantidad de observaciones por cada categoría.\n",
    "\n",
    "---\n",
    "\n",
    "## **3️ Visualización de la Distribución de los Datos**\n",
    "Para comprender la dispersión y la estructura de los datos, generamos diferentes visualizaciones:\n",
    "\n",
    "###  **Histogramas**\n",
    "Los histogramas permiten analizar la distribución de cada variable numérica y detectar sesgos en los datos.\n",
    "\n",
    "###  **Diagramas de Caja (Boxplots)**\n",
    "Los boxplots ayudan a identificar valores atípicos y visualizar la dispersión de los datos.\n",
    "\n",
    "---\n",
    "\n",
    "## **4️ Análisis de Correlación entre Variables**\n",
    "Para evaluar relaciones entre variables y posibles colinealidades, generamos:\n",
    "\n",
    "- **Matriz de correlación:** Muestra el coeficiente de correlación entre las variables numéricas.\n",
    "- **Mapa de calor:** Representación visual de la matriz de correlación con `seaborn.heatmap`.\n",
    "\n",
    "---\n",
    "\n",
    "## **5️ Conclusiones del Análisis Exploratorio**\n",
    "A partir de los gráficos y estadísticas obtenidos, podemos destacar los siguientes puntos clave:\n",
    "\n",
    "1. **Las estadísticas descriptivas** nos permiten ver la media, mediana, moda y dispersión de los datos.\n",
    "2. **Histogramas y diagramas de caja** ayudan a identificar valores extremos y patrones de distribución.\n",
    "3. **Los gráficos de dispersión** pueden mostrar relaciones entre pares de variables relevantes.\n",
    "4. **La matriz de correlación** indica posibles relaciones entre características.\n",
    "5. **Valores atípicos** deben ser analizados y tratados adecuadamente antes del modelado.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar valores nulos, duplicados y tipos de datos\n",
    "print(\"Valores nulos por columna:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\nValores duplicados:\")\n",
    "print(df.duplicated().sum())\n",
    "print(\"\\nTipos de datos:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Limpieza de datos\n",
    "# Eliminar valores duplicados\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Imputar valores nulos\n",
    "df['TotalCharges'] = df['TotalCharges'].replace(\" \", np.nan).astype(float)\n",
    "df['TotalCharges'] = df['TotalCharges'].fillna(df['TotalCharges'].mean())\n",
    "\n",
    "# Transformación de datos\n",
    "# Convertir variables categóricas en variables numéricas\n",
    "label_encoders = {}\n",
    "for column in categorical_columns:\n",
    "    if df[column].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        df[column] = le.fit_transform(df[column])\n",
    "        label_encoders[column] = le\n",
    "\n",
    "# Normalización y estandarización\n",
    "scaler = StandardScaler()\n",
    "df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
    "\n",
    "# Informe de los pasos realizados\n",
    "print(\"\\nInforme de limpieza y transformación:\")\n",
    "print(\"1. Valores duplicados eliminados.\")\n",
    "print(\"2. Valores nulos imputados en la columna 'TotalCharges'.\")\n",
    "print(\"3. Variables categóricas convertidas en variables numéricas.\")\n",
    "print(\"4. Variables numéricas normalizadas usando Z-score Normalization.\")\n",
    "\n",
    "# Visualización de datos antes y después del preprocesamiento\n",
    "print(\"\\nDatos antes del preprocesamiento:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nDatos después del preprocesamiento:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Preprocesamiento de Datos\n",
    "\n",
    "En esta sección, me encargo de **limpiar y transformar** los datos para asegurar que estén en el formato adecuado antes de entrenar cualquier modelo de machine learning.\n",
    "\n",
    "---\n",
    "\n",
    "## **1️ Identificación de Problemas en los Datos**\n",
    "Antes de aplicar transformaciones, lo primero que hago es revisar la calidad de los datos. Para ello, verifico:\n",
    "\n",
    "- **Valores nulos**: Identifico si hay datos faltantes en alguna columna.\n",
    "- **Valores duplicados**: Detecto si hay filas repetidas que podrían afectar el análisis.\n",
    "- **Tipos de datos**: Me aseguro de que cada variable tenga el formato adecuado (números, categorías, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "## **2️ Limpieza de Datos**\n",
    "###  Eliminación de Duplicados  \n",
    "Si encuentro filas duplicadas en el dataset, las elimino para evitar que influyan negativamente en el modelo.\n",
    "\n",
    "###  Imputación de Valores Nulos  \n",
    "En la columna `TotalCharges`, detecté algunos valores vacíos representados como `\" \"`. Lo que hago es:\n",
    "\n",
    "1. Convertir esos valores en `NaN` para tratarlos correctamente.\n",
    "2. Rellenar los valores `NaN` con la **media** de la columna, asegurando que no haya datos faltantes.\n",
    "\n",
    "---\n",
    "\n",
    "## **3️ Transformación de Datos**\n",
    "###  Conversión de Variables Categóricas  \n",
    "Dado que los modelos de machine learning no pueden manejar directamente variables categóricas, convierto estas variables en valores numéricos utilizando `LabelEncoder`.  \n",
    "\n",
    "Cada categoría recibe un número único, lo que permite que los modelos interpreten la información correctamente.\n",
    "\n",
    "---\n",
    "\n",
    "## **4️ Normalización y Estandarización**\n",
    "Para que los modelos trabajen mejor con las variables numéricas (`tenure`, `MonthlyCharges`, `TotalCharges`), aplico `StandardScaler`.  \n",
    "Esto **normaliza** los valores usando la técnica **Z-score Normalization**, que centra los datos en media 0 y varianza 1.  \n",
    "\n",
    " **Ventaja:** Evita que una variable con valores muy grandes (ejemplo: `TotalCharges`) domine sobre otras con valores más pequeños.\n",
    "\n",
    "---\n",
    "\n",
    "## **5️ Resumen del Preprocesamiento**\n",
    "Después de aplicar los pasos anteriores, genero un informe de los cambios realizados:\n",
    "\n",
    "1. **Valores duplicados eliminados.**\n",
    "2. **Valores nulos imputados en la columna `TotalCharges`.**\n",
    "3. **Variables categóricas convertidas en valores numéricos.**\n",
    "4. **Variables numéricas normalizadas usando `Z-score Normalization`.**\n",
    "\n",
    "---\n",
    "\n",
    "## **6️ Visualización de los Datos Antes y Después**\n",
    "Finalmente, muestro un resumen del dataset antes y después del preprocesamiento para asegurarme de que las transformaciones se aplicaron correctamente.\n",
    "\n",
    "Este proceso es clave para garantizar que los datos sean adecuados para el entrenamiento de modelos de machine learning y evitar sesgos que puedan afectar el rendimiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Definir las características (X) y la variable objetivo (y)\n",
    "X = df.drop(columns=['Churn', 'customerID'])\n",
    "y = df['Churn']\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f'Tamaño del conjunto de entrenamiento: {X_train.shape[0]}')\n",
    "print(f'Tamaño del conjunto de prueba: {X_test.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Definir los modelos y sus hiperparámetros\n",
    "models = {\n",
    "    'RandomForest': {\n",
    "        'model': RandomForestClassifier(random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [None, 10, 20, 30],\n",
    "            'min_samples_split': [2, 5, 10]\n",
    "        }\n",
    "    },\n",
    "    'GradientBoosting': {\n",
    "        'model': GradientBoostingClassifier(random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'max_depth': [3, 5, 7]\n",
    "        }\n",
    "    },\n",
    "    'SVC': {\n",
    "        'model': SVC(random_state=42),\n",
    "        'params': {\n",
    "            'C': [0.1, 1, 10],\n",
    "            'kernel': ['linear', 'rbf', 'poly'],\n",
    "            'gamma': ['scale', 'auto']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Entrenar y optimizar los modelos\n",
    "best_models = {}\n",
    "for name, model_info in models.items():\n",
    "    grid_search = GridSearchCV(model_info['model'], model_info['params'], cv=5, n_jobs=-1, scoring='accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_models[name] = grid_search.best_estimator_\n",
    "    print(f\"Best parameters for {name}: {grid_search.best_params_}\")\n",
    "    print(f\"Best score for {name}: {grid_search.best_score_}\")\n",
    "\n",
    "# Evaluar los mejores modelos en el conjunto de prueba\n",
    "for name, model in best_models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"Classification report for {name}:\\n{classification_report(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Entrenamiento y Optimización de Modelos de Machine Learning\n",
    "\n",
    "En esta sección, me encargo de **definir, entrenar y optimizar** varios modelos de clasificación para predecir la fuga de clientes (*churn*). Utilizo **búsqueda de hiperparámetros** para obtener el mejor rendimiento posible.\n",
    "\n",
    "---\n",
    "\n",
    "## **1️ Selección de Modelos**\n",
    "Para abordar este problema, seleccioné tres algoritmos de clasificación populares:\n",
    "\n",
    "- **Random Forest:** Modelo basado en múltiples árboles de decisión, ideal para manejar datos con relaciones no lineales.\n",
    "- **Gradient Boosting:** Algoritmo de boosting que ajusta errores progresivamente para mejorar la precisión.\n",
    "- **SVM (Support Vector Machine):** Modelo basado en la maximización de márgenes entre clases.\n",
    "\n",
    "Cada uno de estos modelos tiene ventajas específicas:\n",
    "✅ **Random Forest** es robusto y maneja bien datos con muchas características.  \n",
    "✅ **Gradient Boosting** suele lograr alta precisión optimizando el error en cada iteración.  \n",
    "✅ **SVM** es útil cuando los datos no son linealmente separables y requiere pocos parámetros de ajuste.  \n",
    "\n",
    "---\n",
    "\n",
    "## **2️ Definición de Hiperparámetros**\n",
    "Para cada modelo, defino una serie de hiperparámetros que quiero optimizar:\n",
    "\n",
    "- **Random Forest:**\n",
    "  - `n_estimators`: Número de árboles en el bosque.\n",
    "  - `max_depth`: Profundidad máxima de cada árbol.\n",
    "  - `min_samples_split`: Mínimo de muestras necesarias para dividir un nodo.\n",
    "\n",
    "- **Gradient Boosting:**\n",
    "  - `n_estimators`: Número de árboles a entrenar.\n",
    "  - `learning_rate`: Controla cuánto cambia el modelo con cada iteración.\n",
    "  - `max_depth`: Profundidad de los árboles en la fase de boosting.\n",
    "\n",
    "- **SVC (Support Vector Machine):**\n",
    "  - `C`: Parámetro de regularización (más alto, menos penalización).\n",
    "  - `kernel`: Define la función del hiperplano (lineal, polinomial o radial).\n",
    "  - `gamma`: Ajusta la influencia de los puntos de entrenamiento.\n",
    "\n",
    "Estos hiperparámetros son clave porque afectan directamente el rendimiento del modelo.\n",
    "\n",
    "---\n",
    "\n",
    "## **3️ Entrenamiento y Optimización con GridSearchCV**\n",
    "Para encontrar los mejores hiperparámetros, utilizo **GridSearchCV**, que prueba múltiples combinaciones y selecciona la mejor configuración.  \n",
    "\n",
    " **Ventaja de GridSearchCV:** Evalúa automáticamente cada combinación con validación cruzada (`cv=5`), lo que asegura que el modelo no esté sobreajustado.\n",
    "\n",
    "El proceso consiste en:\n",
    "1. Definir los modelos y sus hiperparámetros.\n",
    "2. Aplicar `GridSearchCV` para buscar la mejor combinación.\n",
    "3. Guardar el mejor modelo para cada algoritmo.\n",
    "\n",
    "Después de entrenar cada modelo, imprimo los **mejores hiperparámetros** y la **mejor puntuación** obtenida durante la validación cruzada.\n",
    "\n",
    "---\n",
    "\n",
    "## **4️ Evaluación de los Modelos**\n",
    "Una vez que tengo los modelos optimizados, los pruebo en el **conjunto de prueba (`X_test`)** para evaluar su rendimiento real.\n",
    "\n",
    "Para esto, uso `classification_report`, que me proporciona métricas clave como:\n",
    "- **Precisión (`precision`)**: Cuántos de los clientes que predije como \"fugados\" realmente lo son.\n",
    "- **Sensibilidad (`recall`)**: Cuántos de los clientes fugados reales fueron detectados correctamente.\n",
    "- **F1-score**: Un balance entre precisión y sensibilidad.\n",
    "- **Exactitud (`accuracy`)**: Porcentaje total de predicciones correctas.\n",
    "\n",
    "Cada modelo genera un informe con estas métricas, lo que me permite comparar su desempeño y elegir el más adecuado.\n",
    "\n",
    "---\n",
    "\n",
    "## **5️ Conclusión**\n",
    "Después de evaluar los modelos, puedo determinar cuál es el mejor predictor para detectar clientes con riesgo de fuga.\n",
    "\n",
    " **Posibles mejoras futuras:**\n",
    "- Evaluar otros modelos como **XGBoost** o **Redes Neuronales**.\n",
    "- Usar técnicas de balanceo de clases si los datos están desbalanceados (`SMOTE`).\n",
    "- Aplicar `RandomizedSearchCV` para acelerar la búsqueda de hiperparámetros.\n",
    "\n",
    "Este proceso garantiza que obtenga el modelo más eficiente para ayudar a la empresa a reducir la fuga de clientes y mejorar la retención. 🚀🔥\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Evaluar los mejores modelos en el conjunto de prueba\n",
    "for name, model in best_models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Metrics for {name}:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
